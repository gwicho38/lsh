{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "275e81b3-818c-4798-94a8-204a4910e125",
   "metadata": {},
   "source": [
    "# Model Inference Service Administration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e77259-740a-4231-a6fa-117640932fe3",
   "metadata": {},
   "source": [
    "TODO – WRITE INSTRUCTIONS FOR CHUNK AND UPLOAD [HERE](#Vllmfiles)\n",
    "\n",
    "TODO – ADDRESS ALI'S COMMENT (Add a section for waiting until engine is ready before overriding the routes. PLAT-69764: Enforce that warmup is completed for an MlAtomicPipe.Engine prior to allowing a route to be overridden)\n",
    "\n",
    "TODO – ADDRESS ALI'S COMMENT (Add a section to clarify redeploying an entry with a new configuration might not work because of PLAT-74324: Re-deploying an entry with different configurations doesn't work. the workaround is to create a new model registry entry)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1472b5a5-46e5-4407-8c06-fdafe5241765",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Table of contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e9dc6f-67de-4c84-b094-15273ffd494e",
   "metadata": {},
   "source": [
    "- [Introduction](#Introduction)\n",
    "    - [Definitions](#Definitions)\n",
    "        - [Model Inference Service application](#MISapp)\n",
    "        - [Pipe Registration application](#Pipeapp)\n",
    "        - [Architecture 1 vs. architecture 2](#Architectures)\n",
    "    - [Package dependencies](#Packagedependencies)\n",
    "    - [Prerequisites](#Prereqs)\n",
    "- [Service creation](#Servicecreation)\n",
    "    - [Starting an environment](#Startingenv)\n",
    "    - [Starting the services](#Startingservices)\n",
    "    - [Starting the Model Inference Service](#Startingmodelinferenceservice)\n",
    "    - [Starting a pipe registration application](#Startingpipereg)\n",
    "    - [Starting a Model Registry](#Startingregistry)\n",
    "    - [Starting a test client](#Startingclient)\n",
    "    - [Connecting your client application(s) to the Model Inference Service](#Connecting)\n",
    "- [Model serving](#Serving)\n",
    "    - [Creating a VllmPipe](#Vllmpipe)\n",
    "        - [Creating a VllmPipe using a model from Hugging Face Hub](#Vllmhf)\n",
    "        - [Creating a VllmPipe using model files](#Vllmfiles)\n",
    "    - [Registering a pipe to the Model Registry](#Registering)\n",
    "    - [Pipe deployment](#Deployment)\n",
    "        - [Nodepool creation](#Nodepool)\n",
    "        - [Retreiving and deploying the pipe](#Deploy)\n",
    "- [Route management](#Routes)\n",
    "    - [Changing a route](#Changingroute)\n",
    "    - [Terminating a deployment](#Terminating)\n",
    "- [Monitoring the service](#Monitoring)\n",
    "- [Scaling the service](#Scaling)\n",
    "    - [Scaling up](#Scalingup)\n",
    "    - [Scaling down](#Scalingdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5df24f-f0e9-46c0-93a5-c5dcf7477637",
   "metadata": {},
   "source": [
    "## Introduction <a name=\"Introduction\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75b114e-6c95-443d-8774-81d4d97745cc",
   "metadata": {},
   "source": [
    "This tutorial is for the administration of a Model Inference service. A C3 AI cluster that has an application requiring the usage of a large language model will likely require a Model Inference service to route and generate for text generation requests. In this tutorial, we demonstrate the creation and management of a Model Inference service and other related services. \n",
    "\n",
    "For app developers needing to use calls to an LLM in their application, please refer to the ModelInference-Client tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851cf668-cfe2-4a82-aba2-5fde0b8a5da4",
   "metadata": {},
   "source": [
    "### Definitions <a name=\"Definitions\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e894a94a-d212-477e-b637-35db4d2fb3eb",
   "metadata": {},
   "source": [
    "#### Model Inference Service application <a name=\"MISapp\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f21902e-b9b4-4331-9394-7346f9d128bd",
   "metadata": {},
   "source": [
    "The Model Inference service application (MIS app) is the application used to deploy pipes for LLM completion and route all completion requests to the proper deployments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb259563-0f9f-44d0-8138-141718508174",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Pipe registration application <a name=\"Pipeapp\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b14875-5a49-4038-aac5-f552652a638d",
   "metadata": {},
   "source": [
    "The pipe registration application is the application used for registering pipes to the Model Registry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42202a5d-275f-41a7-920e-a8396cb3f893",
   "metadata": {},
   "source": [
    "#### Architecture 1 vs. architecture 2 <a name=\"Architectures\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62285b8-8963-4a20-a9a6-a44f44cea525",
   "metadata": {},
   "source": [
    "Depending on your team's preferences, you may want to opt for one these two architectures:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432d9db9-68da-4040-b721-fb3d5b10c19a",
   "metadata": {},
   "source": [
    "- Architecture 1: The Model Inference service application and the pipe registration application are *separate applications*.\n",
    "- Architecture 2: The Model Inference service application and the pipe registration application are the *same application*.\n",
    "\n",
    "In the diagrams in this tutorial, we assume that is a GenAI application that will be requesting LLM text generation.\n",
    "\n",
    "Architecture 1 | Architecture 2\n",
    "- | - \n",
    "![Architecture 1](architecture1.jpg) | ![alt](architecture2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61297916-f250-4d50-af17-d3131df65d77",
   "metadata": {},
   "source": [
    "### Package dependencies <a name=\"Packagedependencies\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3937420-525a-4947-870c-3129cd9be58f",
   "metadata": {},
   "source": [
    "The packages relevant to this tutorial are:\n",
    "- `modelInferenceService` : The package used for creating the Model Inference service application.\n",
    "- `modelInferencePipes` :  The package used for creating the pipe registration application.\n",
    "- `modelInference` : The package used for the client of the Model Inference service, allowing it to use the completion API.\n",
    "- `modelRegistryService` : The package used for creating a Model Registry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4617b5d4-6a22-4992-9312-e10ac712f99a",
   "metadata": {},
   "source": [
    "In architecture 1, the Model Inference service application depends on the `modelInferenceService` package, which depends on the `modelInferencePipes` package. The pipe registration application only depends on the `modelInferencePipes` package. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867aeb95-0008-4824-8d6d-28572a5382d2",
   "metadata": {},
   "source": [
    "In architecture 2, the Model Inference service application depends on the `modelInferenceService` package, and there is no pipe registration application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246fa9ce-b7e9-40a3-930e-fad3d53bf2d5",
   "metadata": {},
   "source": [
    "In both architectures, the GenAI application depends on the `genaibase` package and should depend on the `modelInference` package to support using the Model Inference Service for LLM text generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d8a7cb-57c4-47cd-bdb5-964e9cbdc272",
   "metadata": {},
   "source": [
    "Architecture 1 | Architecture 2\n",
    "- | - \n",
    "![Architecture 1](architecture1packages.jpg) | ![alt](architecture2packages.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae46578-2d9c-4b65-be82-dfb526731521",
   "metadata": {},
   "source": [
    "### Prerequisites <a name=\"Prereqs\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e84af9-51c1-450f-9d3f-a0b0bc122502",
   "metadata": {},
   "source": [
    "The Kubernetes nodepools must be configured with the required resources (GPUs, CPUs, memory, disk, etc.). This is typically done by Operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d75c06a-6b86-4f3e-b259-575e69bf8f56",
   "metadata": {},
   "source": [
    "## Service creation <a name=\"Servicecreation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4b9397-8921-45e3-a264-d9a13983bde9",
   "metadata": {},
   "source": [
    "### Starting an environment <a name=\"Startingenv\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49431189-2d8f-4bdd-ad4f-f3d803f1c090",
   "metadata": {},
   "source": [
    "From C3 AI Studio, start a multi-node environment called `inference` or a name of choice with server version `8.3.1` or greater. This environment will contain the applications needed to serve LLM completions to another application in the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9462a9-e780-4060-9dc4-4b42848c9866",
   "metadata": {},
   "source": [
    "### Starting the services <a name=\"Startingservices\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc5b843-abf3-4e2e-b8de-bdd700f6913e",
   "metadata": {},
   "source": [
    "Begin by accessing the static console for the multi-node environment you've created. Generally, this is accessed through a URL of the form `https://<cluster_name>.cloud/<environment_name>/c3/static/console/index.html`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778ed31e-393e-4f71-93ac-200651e01985",
   "metadata": {},
   "source": [
    "#### Starting the Model Inference Service <a name=\"Startingmodelinferenceservice\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3a28b4-73db-48f6-9bba-0d50d95491a8",
   "metadata": {},
   "source": [
    "Open Chrome DevTools, and start the application for the Model Inference service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfd1ac5-c55e-4f5e-b902-fde7064b133b",
   "metadata": {},
   "source": [
    "```\n",
    "var rootPkg = \"modelInferenceService\"\n",
    "var appName = \"service\"\n",
    "C3.env().startApp({rootPkg:rootPkg, name:appName})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2290c0-b595-41b0-b539-bb7497e8b360",
   "metadata": {},
   "source": [
    "#### Starting a pipe registration application <a name=\"Startingpipereg\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70547f4-1721-4ddd-bcd5-32961595e3a8",
   "metadata": {},
   "source": [
    "Next, if you decided that architecture 1 (INSERT LINK) is best for your use case, you also need to start the pipe registration app."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82e3147-f923-43be-bcdf-b1a4ac82263a",
   "metadata": {},
   "source": [
    "```\n",
    "var rootPkg = \"modelInferencePipes\"\n",
    "var appName = \"pipesapp\"\n",
    "C3.env().startApp({rootPkg:rootPkg, name:appName})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e8f745-d467-43c3-ba19-25c2b2e5aed8",
   "metadata": {},
   "source": [
    "#### Starting a Model Registry <a name=\"Startingregistry\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676c1beb-e68a-4da4-83fe-3ce233910a80",
   "metadata": {},
   "source": [
    "The Model Registry service should already be available in the cluster when AI Studio is deployed. Optionally, you can create a separate Model Registry for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a0dbae-8e5c-4887-b44c-386d525f880f",
   "metadata": {},
   "source": [
    "```\n",
    "var rootPkg = \"modelRegistryService\"\n",
    "var appName = \"registryservice\"\n",
    "C3.env().startApp({rootPkg:rootPkg, name:appName})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd765c0d-3996-4faf-ab88-e86911689f5c",
   "metadata": {},
   "source": [
    "#### Starting a test client <a name=\"Startingclient\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edd5498-1c0e-41ba-8456-96ac30d58e32",
   "metadata": {},
   "source": [
    "If you'd like to set up an application to act as the client for the Model Inference service for testing, you need to start a client application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252024f3-5769-488f-b6de-310c340a0010",
   "metadata": {},
   "source": [
    "```\n",
    "var rootPkg = \"modelInference\"\n",
    "var appName = \"client\"\n",
    "C3.env().startApp({rootPkg:rootPkg, name:appName})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4713a6db-1f4b-4d53-bbb7-bb104b28364d",
   "metadata": {},
   "source": [
    "#### Connecting your client application(s) to the Model Inference Service <a name=\"Connecting\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f4f011-b7a2-4279-9d66-50b7968c8a8f",
   "metadata": {},
   "source": [
    "Finally, you must configure your client application to be able to connect to the Model Inference service application to request completions. In the case in which you're connecting to the test client application you created above, you would run the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db486ef0-1d62-49b5-89a3-e7b7b05ff4f6",
   "metadata": {},
   "source": [
    "```\n",
    "var name = \"ModelInference\"\n",
    "var serviceApp = App.forName('service')\n",
    "Microservice.Config.forName(name).setConfigValue(\"appId\", serviceApp.id, ConfigOverride.ENV)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5517cf-d06e-4481-904b-a5c54014c5c8",
   "metadata": {},
   "source": [
    "Since we're using `ConfigOverride.ENV`, this will not only set the microservice configuration for the client application named \"ModelInference\", but it will also set the configuration for every application in the environment. To make sure that every application in the cluster uses this Model Inference service for LLM text generation, you can run the same with `ConfigOverride.CLUSTER`. This is not recommended."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1e8853-c182-4489-a1ff-dad2a206131f",
   "metadata": {},
   "source": [
    "In the case in which you'd like to connect an actual application in a different environment in the cluster to the Model Inference service (perhaps a GenAI application), you would need to access the static console of that application and run the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdc8791-71bc-4959-a486-ff8a00671d8f",
   "metadata": {},
   "source": [
    "```\n",
    "var name = \"ModelInference\"\n",
    "var serviceAppId = '<CLUSTERNAME>-inference-service'\n",
    "Microservice.Config.forName(name).setConfigValue(\"appId\", serviceAppId, ConfigOverride.APP) # Note I'm doing it for APP so no other app is affected.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be7c88d-95fd-4223-92bb-d75939798017",
   "metadata": {},
   "source": [
    "And similarly, if you set up the separate Model Registry for testing, you must similarly configure the client to reach out to that Model Registry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c848bc-a759-416b-bc8b-822bd2c94c66",
   "metadata": {},
   "source": [
    "```\n",
    "var name = \"ModelRegistry\"\n",
    "var serviceApp = App.forName('registryservice')\n",
    "Microservice.Config.forName(name).setConfigValue(\"appId\", serviceApp.id, ConfigOverride.ENV)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea0cc2c-7c47-44ab-8511-0f9b69deee20",
   "metadata": {},
   "source": [
    "## Model serving <a name=\"Serving\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ad826c-20b3-4539-8d44-cdf4bb46a03b",
   "metadata": {},
   "source": [
    "In order to serve LLM for text generation on the C3 AI Platform, you need to\n",
    "- Create and register a `VllmPipe` to the Model Registry.\n",
    "- Create an appropriately sized nodepool for the deployment of the model.\n",
    "- Deploy the `VllmPipe` to that nodepool.\n",
    "- Set a route that can be used to access the deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a74cd15-2dd9-44a0-bfd9-bb9e3abaf172",
   "metadata": {},
   "source": [
    "### Creating a `VllmPipe` <a name=\"Vllmpipe\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba16b2c-2ff6-4459-a468-f863b9a50c70",
   "metadata": {},
   "source": [
    "A `VllmPipe` can be created by either downloading the files from Hugging Face Hub or by using model files you already have downloaded. Please note that a `VllmPipe` can only be created with specific, supported models. Currently, Falcon-40B is the only officially tested and supported on the C3 AI Platform, but theoretically supported models can be found [here](https://vllm.readthedocs.io/en/latest/models/supported_models.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c577eb16-12bb-4d2c-a05d-0e6eb044d959",
   "metadata": {},
   "source": [
    "#### Creating a `VllmPipe` using a model from Hugging Face Hub <a name=\"Vllmhf\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ea0eb0-b76f-4739-88c9-353b30adad6b",
   "metadata": {},
   "source": [
    "In Jupyter, we create a `VllmPipe` by specifying\n",
    "- `modelId` : This is the unique identifier of the model from Hugging Face Hub.\n",
    "- `tensorParallelSize` : This is how many GPUs across which you would like to hold the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e30599-629e-4943-8281-415559dc40c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelId = \"tiiuae/falcon-40b\"\n",
    "pipe = c3.VllmPipe(modelId=modelId, tensorParallelSize=8).withDefaults()\n",
    "pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531d579e-e5b9-4a05-b9f0-e6a2dfaceffc",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Creating a `VllmPipe` using model files <a name=\"Vllmfiles\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d298f2-7714-43c4-883f-9055d425f0a8",
   "metadata": {},
   "source": [
    "If you have model files locally, you can create the `VllmPipe` by specifying the `local_path` for the directory containing the model files and running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bce186-243b-47b8-a10f-f0903c6f34e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = c3.VllmPipe.convert(local_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef7796a-d098-4761-be0b-bba1de0a8636",
   "metadata": {},
   "source": [
    "Or, if the model files are located in a filesystem mounted to the cluster, you can create the `VllmPipe` by specifying the `filesystem_path` and running the following: TODO – WRITE INSTRUCTIONS FOR CHUNK AND UPLOAD HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cebb41-e799-4329-99a3-b5a47050dd3b",
   "metadata": {},
   "source": [
    "### Registering a pipe to the Model Registry <a name=\"Registering\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d12fde1-7d14-4d75-95c1-5160e84affd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = c3.VllmPipe(modelUrl=filesystem_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068aa028-adcf-43aa-b8f1-82e83c559dd1",
   "metadata": {},
   "source": [
    "To serve LLM completions on the C3 AI Platform, a `VllmPipe` associated with an LLM must be registered to the Model Registry. This pipe registration must be done from one of the following applications depending on the choice of architecture:\n",
    "- The pipe registration application (architecture 1) or\n",
    "- The Model Inference service application (architecture 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5967f1d-f6aa-45b7-8c82-05591e3b261d",
   "metadata": {},
   "outputs": [],
   "source": [
    "c3.ModelRegistry.registerMlPipe(pipe, \"f40b\", \"falcon40b-8gpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4af9343-6abd-45f2-9ebc-907a15c3f675",
   "metadata": {},
   "source": [
    "### Pipe deployment <a name=\"Deployment\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db3d69b-96ea-4bc4-808e-441685bf22b2",
   "metadata": {},
   "source": [
    "#### Nodepool creation <a name=\"Nodepool\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b11f44-fe21-42de-bc59-0ca4c9002658",
   "metadata": {},
   "source": [
    "In order to deploy the pipe, we need to create a nodepool to which we will deploy it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0100a7-1ce4-435c-aa8f-29677b3e9e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "hwProfile = c3.HardwareProfile.upsertProfile({\n",
    "    \"name\": '8x40a100_90cpu_600mem',\n",
    "    \"cpu\": 90,\n",
    "    \"memoryMb\": 600000,\n",
    "    \"gpu\": 4,\n",
    "    \"gpuKind\": 'nvidia-a100-40gb-8',\n",
    "    \"gpuVendor\": 'nvidia',\n",
    "    \"diskGb\" : 1000\n",
    "});\n",
    "    \n",
    "name = '8xa100'\n",
    "    \n",
    "c3.app().configureNodePool(name,                    # name of the node pool to configure \n",
    "                          1,                        # sets the target node count\n",
    "                          1,                        # sets the minimum node count\n",
    "                          1,                        # sets the maximum node count\n",
    "                          hwProfile,                # sets the hardware profile\n",
    "                          [c3.Server.Role.SERVICE], # sets the server role that this node pool will function as                  \n",
    "                          False,                    # optional - specifies whether autoscaling should be enabled\n",
    "                          0.10                      # optional - JvmSpec\n",
    "                          ).update();   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764a12b8-a0fc-43c6-ac62-ffc86325d12a",
   "metadata": {},
   "source": [
    "#### Retrieving and deploying the pipe <a name=\"Deploy\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0ea768-518f-46b8-b514-c9ed46d44609",
   "metadata": {},
   "source": [
    "We retrive the latest entry for the `\"f40b\"` URI from the Model Registry. In order to deploy the pipe successfully, this must be performed in the Model Inference service application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deeb09ab-602b-446f-ba90-cea636edd85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vers = c3.ModelRegistry.listVersions(\"f40b\").objs\n",
    "entry = vers[0]\n",
    "entry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f73715-fd82-4004-b1d1-e57e3d0b5847",
   "metadata": {},
   "source": [
    "We use the `deploy()` API to deploy the entry to the nodepool we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3838998-3544-46fa-bee5-1e432dc6d8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "c3.ModelInference.deploy(entry, nodePool=\"8xa100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28e7910-f1e8-43ba-9e11-e9be3ff36506",
   "metadata": {},
   "source": [
    "Finally, we set a route that can be used to access this deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c538b2-1b4b-4f4f-bd94-c04e45e32eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "c3.ModelInference.setRoute(entry, \"falcon40b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9d6385-58e3-4466-8b8a-b20747ebe4e4",
   "metadata": {},
   "source": [
    "Now the client application is able to use the `c3.ModelInference.completion()` API with this route to request test generation from this Falcon-40B LLM deployment. To test this, try running the following line from the client application in Jupyter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec96f21e-96ca-4d53-b665-8a3337e583f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "c3.ModelInference.completion(route=\"falcon40b\", prompts=[\"hello\"], params = {'max_tokens' : 128})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def6f113-5f5a-4de5-98be-6f7886fdb825",
   "metadata": {},
   "source": [
    "Note that the `c3.ModelInference.completion()` API is also able to be called from the Model Inference service application for testing purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd1da07-895b-45e7-b94c-1b7a11afc4a3",
   "metadata": {},
   "source": [
    "## Route management <a name=\"Routes\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cda1e9-23cf-46e9-bd86-e85a5920cc30",
   "metadata": {},
   "source": [
    "It may become necessary to change/upgrade an LLM you are using. This can be done by deploying a new `VllmPipe` (if not already deployed) and then changing the route to use the new deployment instead of the old one. In this section, we demonstrate an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a54527-805d-4e26-b30e-050a3ca45bb6",
   "metadata": {},
   "source": [
    "### Changing a route <a name=\"Changingroute\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e0f2f6-b7ef-4691-8f21-a8719f0a5fa0",
   "metadata": {},
   "source": [
    "Let's assume that we are replacing the Falcon-40B model we deployed above with a new Falcon-40B model. Let's also assume that we've registered a `VllmPipe` for that model with the same uri `\"f40b\"` and created a new nodepool named `\"8xa100_new\"` for the deployment. We can run the following code to retrive the latest entry in the Model Registry for that URI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0ea160-8e1b-4e3f-95b3-14f5198fbb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "vers = c3.ModelRegistry.listVersions(\"f40b\").objs\n",
    "new_entry = vers[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed21621e-c101-4c19-a53d-becf3bc93a94",
   "metadata": {},
   "source": [
    "We deploy the new entry to the nodepool we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e956f352-b781-40bf-9fa8-a5477dd7f3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "c3.ModelInference.deploy(new_entry, nodePool=\"8xa100_new\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809e3381-c7d9-41b2-a467-b45654042d28",
   "metadata": {},
   "source": [
    "And now that we've retreived and deployed the latest entry, we can simply change the route we were using before (`\"falcon40b\"`) to use this new entry instead of the old one, thereby upgrading the route to use our newer Falcon-40B large language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a64dba-7db4-47c3-b667-2d8cff2d262f",
   "metadata": {},
   "outputs": [],
   "source": [
    "c3.ModelInference.setRoute(pipeEntry=new_entry, route=\"falcon40b\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb716ba7-d049-4c02-8096-70c965f40643",
   "metadata": {},
   "source": [
    "### Terminating a deployment <a name=\"Terminating\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a184837-9faa-4588-878a-dfb4314c8536",
   "metadata": {},
   "source": [
    "If we no longer need the old Falcon-40B deployment, we can terminate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6359fb-e838-4d97-bff4-e4d59ebe3d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Terminate the engine\n",
    "c3.ModelInference.terminate(entry, confirm=True, removeRoutes=True)\n",
    "\n",
    "# Terminate the nodepool\n",
    "c3.app().nodePool('8xa100').terminate()\n",
    "c3.App.NodePool.Config.forName('8xa100').clearConfigAndSecretAllOverrides()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efc3bcd-da06-43cf-915e-8cebdec0a519",
   "metadata": {},
   "source": [
    "## Monitoring the service <a name=\"Monitoring\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28ec0b0-ee55-4fe4-9b00-f335d88ec8b8",
   "metadata": {},
   "source": [
    "We can run the `c3.ModelInference.summary()` API to monitor the underlying engines, threadpools, and threads on our Model Inference service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6a8756-35d1-4e8c-94b3-e2bdcd5acf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "c3.ModelInference.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc45b3d-e365-45a5-9590-32e38a3a9b6b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Scaling the service <a name=\"Scaling\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2a4fb5-d5ee-4221-b8c6-1ae2a162f3d0",
   "metadata": {},
   "source": [
    "It may become necessary to scale the service when request volume changes. Currently, this is done manually by changing the number of nodes in the nodepool corresponding to the pipe deployment. In the world of large language models, adding a new node to the nodepool corresponding to the pipe deployment is sometimes referred to as \"creating a replica of the model,\" as the new node in the nodepool will have its own copy of the model in its GPU memory. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28047cba-5ce3-49fe-b77d-c2ae98be4931",
   "metadata": {},
   "source": [
    "### Scaling up <a name=\"Scalingup\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdf8af9-7da3-4bbd-bcc6-2afc13837668",
   "metadata": {},
   "source": [
    "In order to \"create a replica,\" or scale up a deployment, we can run the following to increase the number of nodes in the nodepool from one to two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b613d42-5d95-4a10-baf1-1e407685c873",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodepool_name = \"8xa100_new\"\n",
    "num_nodes = 2\n",
    "\n",
    "c3.app().nodePool(nodepool_name).setNodeCount(num_nodes, num_nodes, num_nodes).update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a4ae2b-f2ee-4f56-ac3b-e59651d7cc06",
   "metadata": {},
   "source": [
    "We can get updates on the scaling by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40a4c01-a63e-4cad-a526-97c75b3de1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "np1 = c3.app().nodePool(nodepool_name)\n",
    "assert not np1.isReady()\n",
    "\n",
    "num = 0\n",
    "while not np1.isReady(): \n",
    "    res1 = c3.ModelInference.completion(route=\"falcon40b\", prompts=[\"hello\"], params={})\n",
    "    text1 = assert_text_not_none(res1)\n",
    "    print(f'Nodepool {nodepool1_name} is not ready.')\n",
    "    print(f'Result of calling `completion`: {text1}')\n",
    "    num += 1\n",
    "    time.sleep(60)\n",
    "\n",
    "assert num > 0\n",
    "    \n",
    "print(f'Nodepool {nodepool1_name} scaled up and is ready!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece98da2-c6ec-4008-b72b-cc7a152afdaf",
   "metadata": {},
   "source": [
    "This creates a new node in the nodepool we named `\"8xa100_new\"` that will serve the same model that we have deployed to that nodepool. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d79c22c-023f-4a81-9d28-b303b6287c4d",
   "metadata": {},
   "source": [
    "### Scaling down <a name=\"Scalingdown\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b6c4a4-1d7d-48f7-86ef-5fca55b32fa2",
   "metadata": {},
   "source": [
    "Scaling down is similar to scaling up. In order to scale down our deployment to one node, we can run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1e78e0-3ebf-4c11-8ae0-40744db97d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodepool_name = \"8xa100_new\"\n",
    "num_nodes = 1\n",
    "\n",
    "c3.app().nodePool(nodepool_name).setNodeCount(num_nodes, num_nodes, num_nodes).update()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
