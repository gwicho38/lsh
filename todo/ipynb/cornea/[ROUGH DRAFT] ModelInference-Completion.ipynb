{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a48c843-c596-4835-99f8-39e187e7a6a3",
   "metadata": {},
   "source": [
    "# Using the Model Inference Service for LLM Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d067af9-7720-4423-b1b6-8db1f7ed8e29",
   "metadata": {},
   "source": [
    "TODO â€“ ADD MATERIAL ON WHAT THE COMPLETION API ACTUALLY RETURNS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1356414-07cc-4212-bb9f-ef9c1c358891",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da2af7d-3cc2-49bd-a739-652965e418d9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Setup functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4686b8f-f24d-4b9a-8e89-5e8ea17deeaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_pipe(model, fromFile=False):\n",
    "    \"\"\"\n",
    "    model: The path or id of the model to use with vLLM\n",
    "    fromFile: if True, call convert using the path specified by `model`.\n",
    "    \"\"\"\n",
    "    if fromFile:\n",
    "        pipe = c3.VllmPipe.convert(model, c3.VllmConvertSpec(trustRemoteCode=True, tensorParallelSize=8))\n",
    "    else:\n",
    "        pipe = c3.VllmPipe(modelId=model, trustRemoteCode=True, tensorParallelSize=8)\n",
    "    return pipe.create(returnInclude='this')\n",
    "\n",
    "def register_pipe_to_model_registry(pipe, uri, description):\n",
    "    \"\"\"\n",
    "    pipe: pipe to register to model registry\n",
    "    uri: uri to register the pipe with\n",
    "    description: description of the pipe\n",
    "    \"\"\"\n",
    "    return c3.ModelRegistry.registerMlPipe(pipe, uri, description)\n",
    "\n",
    "\n",
    "def get_latest_entry(uri):\n",
    "    \"\"\"\n",
    "    uri: uri of model registry entry \n",
    "    \"\"\"\n",
    "    vers = c3.ModelRegistry.listVersions(uri).objs\n",
    "    return vers[0] if len(vers) > 0 else None\n",
    "\n",
    "\n",
    "def create_nodepool(name=\"1t4\"):\n",
    "    hwProfile  = c3.HardwareProfile.upsertProfile({\n",
    "  \"name\": '1xt4_30cpu_115mem',\n",
    "  \"cpu\": 30,\n",
    "  \"memoryMb\": 115000,\n",
    "  \"gpu\": 1,\n",
    "  \"gpuVendor\": \"nvidia\",\n",
    "  \"gpuKind\": \"nvidia-t4\"\n",
    "});\n",
    "    \n",
    "    c3.app().configureNodePool(name,                    # name of the node pool to configure \n",
    "                          1,                                   # sets the target node count\n",
    "                          1,                                   # sets the minimum node count\n",
    "                          1,                                   # sets the maximum node count\n",
    "                          hwProfile,                        # sets the hardware profile\n",
    "                          [c3.Server.Role.SERVICE],               # sets the server role that this node pool will function as                  \n",
    "                          False,                           # optional - specifies whether autoscaling should be enabled\n",
    "                          0.10                              # optional - JvmSpec\n",
    "                          ).update();      \n",
    "\n",
    "    \n",
    "def deployModelForRoute(model, uri, description, nodepool_name, route, fromFile=False):\n",
    "    \"\"\"\n",
    "    uri: The uri to register the pipe with in Model Registry\n",
    "    \"\"\"\n",
    "    entry = get_latest_entry(uri)\n",
    "    if not entry:\n",
    "        pipe = create_pipe(model)\n",
    "        entry = register_pipe_to_model_registry(pipe, uri, description)\n",
    "\n",
    "\n",
    "    nodepool = c3.app().nodePool(nodepool_name)\n",
    "    if nodepool is None:\n",
    "        create_nodepool(nodepool_name)\n",
    "        print(f\"Created the nodepool: {nodepool_name}. Please edit the deployment to configure shared memory on the node\")\n",
    "\n",
    "    c3.ModelInference.deploy(entry, nodePool=nodepool_name)\n",
    "    c3.ModelInference.setRoute(entry, route)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a893eb9e-fd09-4894-92bb-17e5157e32f5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Setting up for completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e943890-bd54-488c-be5b-9276255f89b4",
   "metadata": {},
   "source": [
    "#### Start Required Microservices (Model Registry + Model Inference)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6f340d-7593-4491-abba-591b6949b221",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _start_app_service(env, rootPkg, name):\n",
    "    app = env.startApp(rootPkg=rootPkg, name=rootPkg.lower())\n",
    "    c3.Microservice.Config.forName(name).setConfigValue(\"appId\", app.id, c3.ConfigOverride.CLUSTER)\n",
    "    return app\n",
    "\n",
    "service_env = c3.env()    \n",
    "#inference_app = _start_app_service(service_env, \"modelInferenceService\", \"ModelInference\")\n",
    "registry_app = _start_app_service(service_env, \"modelRegistryService\", \"ModelRegistry\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824d6dfb-09db-4f62-9e3b-a1e1e35f079a",
   "metadata": {},
   "source": [
    "#### Creating and registering the pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aae0ffb-9de0-4720-bb39-2a0e57d45053",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "modelId = \"facebook/opt-125m\"\n",
    "pipe = create_pipe(modelId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f3dee6-b8c6-4111-ba34-a2ab43898179",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "uri = \"opt-125m\"\n",
    "description = \"opt-125m facebook small model vanilla\"\n",
    "\n",
    "register_pipe_to_model_registry(pipe, uri, description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feadfe3-3d37-4a1a-9027-13ffdcd25e14",
   "metadata": {},
   "source": [
    "#### Deploying the pipe and setting the route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc77abd-3440-4677-8d8d-751e24867c0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_nodepool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3008815-f135-4f37-a838-b87a78349442",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "deployModelForRoute(modelId, uri, description, '1t4', 'opt-125m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81e0505-ab4e-4dad-bd55-c473d85c1a74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list(c3.ModelInference.listRoutes())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60264e0e-3328-4874-ab9d-42b92ef7e7d5",
   "metadata": {},
   "source": [
    "## Introduction to the completion API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc26ebaf-4e10-45d5-b620-f4891edb173b",
   "metadata": {},
   "source": [
    "The `c3.ModelInference.completion()` API is the primary way to perform LLM text generation on the C3 Platform. \n",
    "\n",
    "In this tutorial, we detail the inputs to the API and demonstrate how to use it. By the end of the tutorial, you will be able to run your own completion request using the completion API as seen here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c5c8d8-fd6b-4cd5-923f-5bd9657c5af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define route:\n",
    "route = 'opt-125m'\n",
    "\n",
    "# Define prompts:\n",
    "prompt_1 = \"Respond to this question as if you were a computer scientist: What is the difference between interpreted and compiled programming languages?\"\n",
    "prompt_2 = \"Respond to this question as if you were a data scientist: What is the difference between classification and regression?\"\n",
    "prompts = [prompt_1, prompt_2]\n",
    "\n",
    "# Define params:\n",
    "params = {\n",
    "    'max_tokens' : 128,\n",
    "    'temperature' : 0.5,\n",
    "    'n' : 2\n",
    "}\n",
    "\n",
    "# Request LLM responses:\n",
    "c3.ModelInference.completion(route=route, prompts=prompts, params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e0d1c8-ee1e-4d43-accf-8a6c94c89c74",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Inputs to the completion API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f00658-470e-451c-b678-8b0b20c90b5b",
   "metadata": {},
   "source": [
    "The inputs to the `c3.ModelInference.completion()` API are `route` , `prompts` and `params`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec94cc70-c2b4-44e9-81b1-01d8e52e0920",
   "metadata": {},
   "source": [
    "### The `route` input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a27c619-5f1a-4236-9060-d2971eb4dc6e",
   "metadata": {},
   "source": [
    "The `route` input to the `c3.ModelInference.completion()` API is a string. \n",
    "\n",
    "It determines which LLM will be used to generate the responses for your prompts. The `route` input accepts a string that refers to a specific route that the Model Inference Service application administrator has set up for you to use. The `route` input is often a string like `opt-125m`, which would refer to the opt-125m model from Facebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02e7a80-02eb-4424-864d-6245cfebb7f2",
   "metadata": {},
   "source": [
    "### The `prompts` input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0b07fb-2759-463f-8894-631de04da8ed",
   "metadata": {},
   "source": [
    "The `prompts` input to the `c3.ModelInference.completion()` API is an list of strings.\n",
    "\n",
    "It determines the prompts for which the LLM will generate responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a13f6f1-53e4-4204-a1cc-55ec3317a818",
   "metadata": {},
   "source": [
    "### The `params` input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08816406-fca4-42bc-8801-a52271b98a0b",
   "metadata": {},
   "source": [
    "The `params` input to the `c3.ModelInference.completion()` API is a map from string to any.\n",
    "\n",
    "It determines the model-specific parameters that will be used to generate the responses for your prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be36d2d5-cc9c-436b-9b60-15609a8aecbb",
   "metadata": {},
   "source": [
    "## The completion API in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bec359-12c9-46dc-b4ac-112caaab6d0d",
   "metadata": {},
   "source": [
    "Let's go through an example of how you might use the completion API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fb4ce1-f770-412b-b402-218f17d6dee9",
   "metadata": {},
   "source": [
    "### Checking the available routes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842f6610-c0e4-4f5d-b1f5-22619ff4b1f0",
   "metadata": {},
   "source": [
    "The route determines which LLM will be used to generate the responses for your prompts. You can check which routes have been made available for you by the Model Inference Service application administrator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e98938-3d3c-44f8-ae54-e28569ec3ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(c3.ModelInference.listRoutes())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bf777b-4781-45cf-b00c-b924a23c1a11",
   "metadata": {},
   "source": [
    "We can see that the route `opt-125m` is available to generate responses. Thus, we will set our `route`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1418c03f-2f2c-4563-8718-44dbb25b236c",
   "metadata": {},
   "outputs": [],
   "source": [
    "route = 'opt-125m'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93b3e02-0c17-4dad-be5c-ae1d673694ec",
   "metadata": {},
   "source": [
    "### Defining the prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30be494-92c5-4c26-a933-c775eef053f1",
   "metadata": {},
   "source": [
    "Since the completion API accepts a list of strings as the prompts for which the LLM will generate responses, we assemble our list of prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba586291-e1d2-4be4-8745-fd2b8bfa9ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_1 = \"Respond to this question as if you were a computer scientist: What is the difference between interpreted and compiled programming languages?\"\n",
    "prompt_2 = \"Respond to this question as if you were a data scientist: What is the difference between classification and regression?\"\n",
    "prompts = [prompt_1, prompt_2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a469fd17-1830-42f0-8095-626a63a9c845",
   "metadata": {},
   "source": [
    "Please note that it is not recommended to provide more than 10 prompts in one call to the completion API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01120b1-976e-4461-ad5a-dd3f511387c7",
   "metadata": {},
   "source": [
    "### Defining the completion parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2135ad1-0545-4bde-857b-d5a9bbc3002b",
   "metadata": {},
   "source": [
    "For a given model, there will be an assortment of parameters for generating text using that model. For our completions using `opt-125m`, we will use three text generation parameters that are common across many LLMs: `max_tokens`, `temperature`, and `n`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6323bff-8a26-42d4-9576-b99c90923e56",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Setting `max_tokens`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4690a9-6de3-4bfc-bf1f-8aac862cacec",
   "metadata": {},
   "source": [
    "`max_tokens` specifies the maximum number of tokens that the LLM will generate. You can think of tokens roughly as words. In our case, we would like the generated text to have an approximate maximum length of 128 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6de86d-7e0e-4c60-aaa6-466a098cc48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9c79a0-bfa2-4f6a-9582-0ab8d3f3e1b6",
   "metadata": {},
   "source": [
    "#### Setting `temperature`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cfaeda-0669-4ba1-b51a-4dd829043eb9",
   "metadata": {},
   "source": [
    "`temperature` specifies the randomnness or creativity of the LLM's responses. \n",
    "\n",
    "Technically speaking, LLMs do not generate tokens. Rather, to generate a token, the LLM first generates a vector of probabilities, called the logit vector, where each entry in the vector corresponds to the probability that a certain token ought to appear. When this vector is generated, there is a random sampling from the probabilities to output one single token. `temperature` allows you to control the randomness/creativity of the responses by adjusting how the random sampling of tokens from the logit vector works. \n",
    "\n",
    "A temperature of `0` will make the text generated almost deterministic, as it limits the sampling to simply choose the token with the highest probability. Higher temperatures allow the sampling to use tokens with lower probabilities, thus generating more \"creative\" reponses. \n",
    "\n",
    "For our case, we will set the temperature to `0.5`, which should generate reasonable but different responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a7ba18-66a7-41a2-bfb2-e342250c5d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0351a232-d07f-4908-beee-a09ca10da6a9",
   "metadata": {},
   "source": [
    "#### Setting `n`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0284f773-7908-4906-93d3-d0e5e1b61c26",
   "metadata": {},
   "source": [
    "`n` controls the number of responses to generaate per prompt. For example, if I submit two prompt for completion with `n` set to `3`, I will receive three responses for each of my prompts, giving me six responses in total. In our case, we will set `n` to `2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fce376-ed9e-4922-9ed7-7988f3600037",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b922583-da9e-483a-b454-8a7cd15b064a",
   "metadata": {},
   "source": [
    "### Assembling the completion parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d614181-8aed-46ee-b68b-9848546d0d94",
   "metadata": {},
   "source": [
    "We can now assemble the completion parameters we will pass to the completion API. Remember, the `params` input is a map of string to any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d38d6d3-5744-408e-aa66-8b07a6de95f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'max_tokens' : max_tokens,\n",
    "    'temperature' : temperature,\n",
    "    'n' : n\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97e4c06-675e-4176-b01b-c3dd9de50ecf",
   "metadata": {},
   "source": [
    "### Generating responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f04a188-48f8-4524-98c0-3aa44e36d40f",
   "metadata": {},
   "source": [
    "Finally, we can pass what we've defined to the completion API to generate responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a49059-e8d9-4317-b594-2a4f08d8f5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "c3.ModelInference.completion(route=route, prompts=prompts, params=params)"
   ]
  }
 ],
 "metadata": {
  "has_local_update": false,
  "is_local": true,
  "is_remote": true,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "last_sync_time": "2023-10-03T22:33:37.883447"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
