c3:
  #
  # Configurations for the C3 Cluster App (e.g. cluster/env name, DDE mode) upon helm installation.
  # Note that Cluster name cannot be "local" and C3 Env name should typically start as "c3".
  # Also note that if singleNode is enabled, there must be an existing Cassandra/Postgres to support C3/C3.
  #
  cluster:
    name: v8
    environment:
      name: c3
      singleNode: false
  #
  # Configurations for the image that C3 Cluster App should be bootstrapped with.
  # This includes the container registry, repository, image tag, and image pull policies.
  #
  image:
    registry: ci-artifacts.c3.ai
    repository: c3
    tag: 8.3.0
    pullPolicy: IfNotPresent
    pullSecret:
  #
  # The path for the c3 server application logs
  #
  log:
    volumeName: 'c3-server-log'
    mountPath: '/usr/local/share/c3/server/log'
    image:
      registry: ci-artifacts.c3.ai
      repository: busybox
      tag: 1.34
  applicationGroup: c3
  metrics:
    enabled: true
    port: 9404
    targetPort: 9404
    jmxMetricsPort: 9404
  #
  # Configurations for config framework, including mount path and PVC usage.
  #
  configFramework:
    #
    # Boot config refers to configurations that the C3 Cluster App should consume during its bootstrapping phase.
    # This includes Env/App configs, blob storage access/secret keys, app code, app mode etc.
    #
    bootConfig:
      enabled: false
      path: '/usr/local/share/c3/server/bootconfig/'
      credentials:
        configRoot: ''
        vaultRoot: ''
        accessKey: ''
        secretKey: ''
        region: ''
        useNodeCloudIdentity: false
        cloudIdentity: ''
      clusterApp:
        configuredServerVersion: 8.2.0-dev+8807
        code: 1
        mode: prod

    config:
      #
      # The container filesystem path where the config directory will be projected.
      # Note that if boot config is enabled, config will be written to the mount path specified by boot config.
      #
      mountPath: '/usr/local/share/c3/server/config'
      #
      # The local filesystem directory to be mounted by the bootstrap node container and projected as Config Framework
      # 'config' directory. The value will be propagated to other C3 leader, task, and data nodes.
      #
      # local:
      #
      # The NFS share directory to be mounted by the bootstrap node container and projected as Config Framework 'config'
      # directory. This is a mandatory configuration element, a C3 cluster will not properly operate without appropriate
      # Config Framework storage. Either server/path or storageClassName must be specified under this field.
      # These values will be propagated to other C3 leader, task, and data nodes.
      #
      nfs:
        #
        # Specifies whether to save config data in an NFS based storage. Otherwise, config data should be stored in an
        # alternative storage (e.g. blob storage).
        #
        enabled: false
        #  server:
        #  path:
        #
        # Default value for CSI-provisioned NFS storage persistent volume claim. It is only relevant if CSI-provisioned
        # NFS storage is used. If the same PV is used to store both config and vault information, then the
        # persistentVolumeClaimName value should be the same here as in the 'vault' section below.
        #
        persistentVolumeClaimName: 'c3-cf-pvc'

        #
        # Normally a persistent volume name need not be specified, storage class should be sufficient. There are cases
        # though when the persistent volume name must be specified, and the following configuration element can be used
        # in those cases.
        #
        #persistentVolumeName: 'c3-cf-pv'

        #
        # A directory inside the CSI-exposed NFS file system root where the Config Framework config state will be
        # stored. It is only relevant if CSI-provisioned NFS storage is used. This configuration element is important
        # when multiple PVCs use the same persistent volume.
        #
        subPath: 'c3-cf-config'
        #
        # A Kubernetes quantity specifying the minimum amount of storage required for configuration. Projects as
        # Persistent Volume Claim spec.resources.requests.storage.
        #
        storage: 1Gi
    vault:
      #
      # The container filesystem path where the vault directory will be projected.
      # Note that if boot config is enabled, vault will be written to the mount path specified by boot config.
      #
      mountPath: '/usr/local/share/c3/server/vault'
      #
      # The local filesystem directory to be mounted by the bootstrap node container and projected as Config Framework
      # 'vault' directory. The value will be propagated to other C3 leader, task, and data nodes.
      #
      # local:
      #
      # The NFS share directory to be mounted by the bootstrap node container and projected as Config Framework 'vault'
      # directory. This is a mandatory configuration element, a C3 cluster will not properly operate without appropriate
      # Config Framework storage. Either server/path or storageClassName must be specified under this field.
      # These values will be propagated to other C3 leader, task, and data nodes.
      #
      nfs:
        #
        # Specifies whether to save vault data in an NFS based storage. Otherwise, vault data should be stored in an
        # alternative storage (e.g. blob storage).
        #
        enabled: false
        #  server:
        #  path:
        #
        # Default value for CSI-provisioned NFS storage persistent volume claim. It is only relevant if CSI-provisioned
        # NFS storage is used. If the same PV is used to store both config and vault information, then the
        # persistentVolumeClaimName value should be the same here as in the 'config' section above.
        #
        persistentVolumeClaimName: 'c3-cf-pvc'

        #
        # Normally a persistent volume name needs not be specified, storage class should be sufficient. There are cases
        # though when the persistent volume name must be specified, and the following configuration element can be used
        # in those cases.
        #
        #persistentVolumeName: 'c3-cf-pv'

        #
        # A directory inside the CSI-exposed NFS file system root where the Config Framework config state will be
        # stored. It is only relevant if CSI-provisioned NFS storage is used. This configuration element is important
        # when multiple PVCs use the same persistent volume.
        #
        subPath: 'c3-cf-vault'
        #
        # A Kubernetes quantity specifying the minimum amount of storage required for vault. Projects as
        # Persistent Volume Claim spec.resources.requests.storage.
        #
        storage: 1Gi

  #
  # Configurations for c3 Kubernetes service account.
  #
  serviceAccount:
    annotations:
      # eks.amazonaws.com/role-arn
      # iam.gke.io/gcp-service-account
      # azure.workload.identity/client-id
    labels:
      # azure.workload.identity/use

  security:
    podSecurityContext:
      runAsUser: 4444 # If empty, a random UID will be generated
      runAsGroup: 4444 # If empty, a random GID will be generated
      fsGroup: 4444 # If empty, a random fsGroup ID will be generated
      supplementalGroups: [4444] # Additionally supplied GIDs
    containerSecurityContext:
      readOnlyRootFilesystem: false
    auth:
      #
      # If set to 1, the c3server(Task, Master, Data) will authenticate user before granting access to the resources.
      # Disables authentication if set to 0.
      #
      enabled: 0
    #
    # Path to cert issued by k8s service account for adding to pod's java trust store.
    #
    certPath: "/var/run/secrets/kubernetes.io/serviceaccount/ca.crt"
    #
    # Path to token issued by k8s service account.
    #
    tokenPath: "/var/run/secrets/kubernetes.io/serviceaccount/token"
    #
    # Storepass value to set for keytool command when importing k8s cert to pod's java trust store.
    #
    storePass: "changeit"
    #
    # Set to true to deploy role and role binding necessary for various k8s cluster operations.
    # (Ex. telemetry, starting/stopping applications, etc)
    #
    deployRole: false
    #
    # Set to true to deploy cluster role and cluster role binding necessary for various k8s cluster operations.
    # (Ex. jupyterhub etc)
    #
    deployClusterRole: false

  #
  # Ingress configuration that will be propagated to ingress.yaml.
  #
  ingress:
    #
    # Enables the ingress on c3 cluster.
    #
    enabled: false

    #
    # Class of ingress controller to use in k8s Ingress, e.g.: nginx, alb
    #
    class: nginx

    #
    # Host to apply ingress rule on
    #
    host: localhost

    #
    # Install helm chart that will bring nginx-controller pods
    #
    install-nginx-controller: false

    annotations:
      nginx.ingress.kubernetes.io/proxy-connect-timeout: "60"
      nginx.ingress.kubernetes.io/proxy-send-timeout: "3600"
      nginx.ingress.kubernetes.io/proxy-read-timeout: "3600"
      nginx.ingress.kubernetes.io/proxy-body-size: "250m" # maximum payload size, e.g. zipped packages for a tag, a client can send

    serviceName: leader
    servicePort: 8888

  leader:
    security:
      auth:
        #
        # If set to 1, the c3server(Leader) will authenticate user before granting access to the resources.
        # Disables authentication if set to 0.
        #
        enabled: 0
    service:
      name: leader
      apiPort: 80
      #
      # apiPort: false in the overlay turns off the second API port altogether.
      #
      api2Port: 8888
      apiTargetPort: 8888
      proxyPort: 8890
      proxyTargetPort: 8890
      # domain: domain used to register the service Url via external-dns. The url will be comprised of the service namespace + domain
      #         for example ds4.c3-platform-cloud.com
      # Requires external-dns support and AWS configuration. See Configuration documentation.
      loadBalancer:
        # This section determines what load balancer types and properties to configure for the service.
        # The properties are grouped by cloud provider (AWS "eks", Azure "aks", etc).
        eks:
          #  Currently used in AWS. The options are "nlb" or "elb". "nlb" is the default, unless explicitly overridden.
          type: 'nlb'
          #
          # Annotate load balancer to use private subnet
          #
          # loadBalancerInternal: "true"
          #
          # Comma separated security group ids to be bound to the given load balancer
          #
          # securityGroups: "sg-sg-00111111111111111, sg-00222222222222222"
          #
          # AWS ELB backend protocol
          #
          # backendProtocol: tcp
          #
          # AWS ELB connection idle timeout
          #
          # connectionIdleTimeout: "3600"
          #
          # AWS ELB SSL certificate.
          #
          # sslCert: "arn:aws:acm:us-west-2:111111111111:certificate/22222222-3333-4444-5555-666666666666"
          #
          # AWS ELB negotiation policy
          #
          negotiationPolicy: ELBSecurityPolicy-TLS-1-2-2017-01
          #
          # For example a values overlay file will look like:
          # c3:
          #   leader:
          #     service:
          #       domain: c3v8-test.com
          #       loadBalancer:
          #          eks:
          #           type: nlb
          #           securityGroups: sg-0227833175998e4ce
          #
          #
          # TODO PLAT-32734
          #
      annotations:
        # Set additional service annotations in key: value form as shown below
        # FOO1: BAR1
        # FOO2: BAR2

    # If set to true, the C3 docker container endpoint (c3-entrypoint) of the leader's container will send verbose
    # output to stdout/stderr. Useful to troubleshoot container startup issues.
    #
    verboseEntrypoint: true
    #
    # If set to true, the node's JVM will start with debugging enabled and will allow a remote debugger to attach.
    #
    jvmDebug: false
    jvmDebugPort: 7700
    #
    # If set to true, the JVM will start in "suspend" mode, and will continue its boot process only after a remote
    # debugger attaches. Note that option has effect only if JVM debugging is enabled with "jvmDebug: true".
    # Useful to troubleshoot server boot issues. ⚠️ Be aware that the JVM will be suspended indefinitely and the
    # container will not start unless a debugger attaches. Must be set to 'false' in production.
    #
    jvmSuspend: false
    resources:
      requests:
        memory: '4Gi'
        cpu: 1
      limits:
        memory: '4Gi'
        cpu: 1
        #
        # JVM -Xmx value will be calculated by the entrypoint logic by applying the fraction specified below to the
        # memory limit specified in this configuration section
        #
        jvmMaxMemFraction: 0.7
    liveMetadata: false
  task:
    security:
      auth:
        #
        # If set to 1, the c3server(Task) will authenticate user before granting access to the resources.
        # Disables authentication if set to 0.
        #
        enabled: 0
    replicas: 1
    #
    # If set to true, the C3 docker container endpoint (c3-entrypoint) of the tasks's container will send verbose
    # output to stdout/stderr. Useful to troubleshoot container startup issues.
    #
    verboseEntrypoint: true
    #
    # If set to true, the node's JVM will start with debugging enabled and will allow a remote debugger to attach.
    #
    jvmDebug: false
    jvmDebugPort: 7900
    #
    # If set to true, the JVM will start in "suspend" mode, and will continue its boot process only after a remote
    # debugger attaches. Note that option has effect only if JVM debugging is enabled with "jvmDebug: true".
    # Useful to troubleshoot server boot issues. ⚠️ Be aware that the JVM will be suspended indefinitely and the
    # container will not start unless a debugger attaches. Must be set to 'false' in production.
    #
    jvmSuspend: false

    resources:
      requests:
        memory: '4Gi'
        cpu: 1
      limits:
        memory: '4Gi'
        cpu: 4
        #
        # JVM -Xmx value will be calculated by the entrypoint logic by applying the fraction specified below to the
        # memory limit specified in this configuration section
        #
        jvmMaxMemFraction: 0.7
    liveMetadata: false
  data:
    security:
      auth:
        #
        # If set to 1, the c3server(Data) will authenticate user before granting access to the resources.
        # Disables authentication if set to 0.
        #
        enabled: 0
    replicas: 1
    #
    # If set to true, the C3 docker container endpoint (c3-entrypoint) of the tasks's container will send verbose
    # output to stdout/stderr. Useful to troubleshoot container startup issues.
    #
    verboseEntrypoint: true
    #
    # If set to true, the node's JVM will start with debugging enabled and will allow a remote debugger to attach.
    #
    jvmDebug: false
    jvmDebugPort: 7901
    #
    # If set to true, the JVM will start in "suspend" mode, and will continue its boot process only after a remote
    # debugger attaches. Note that option has effect only if JVM debugging is enabled with "jvmDebug: true".
    # Useful to troubleshoot server boot issues. ⚠️ Be aware that the JVM will be suspended indefinitely and the
    # container will not start unless a debugger attaches. Must be set to 'false' in production.
    #
    jvmSuspend: false
    resources:
      requests:
        memory: '6Gi'
        cpu: 1
      limits:
        memory: '6Gi'
        cpu: 1
        #
        # JVM -Xmx value will be calculated by the entrypoint logic by applying the fraction specified below to the
        # memory limit specified in this configuration section
        #
        jvmMaxMemFraction: 0.7
    liveMetadata: false

  #
  # Configuration of the filesystem shared space exposed to all nodes in the environment.
  #
  sharedFileSystems:
    volumeMounts:
        #
        # volumeMount name must match the "name" of the volume to be mounted
        #
      - name: environment-shared
        #
        # The container filesystem mount point. The shared filesystem space will be mounted in the same place on all nodes.
        # The value will be transferred verbatim to container's specification volumeMounts.mountPath element.
        # If this value is updated, the code running inside the container that relies on this location must also be updated.
        #
        mountPath: /environment
        #
        # subPath is used to specify a specific directory of the volume to be mounted.
        # By default this value is empty, indicating the entire volume will be mounted.
        #
        # subPath:
    #
    # Inject volume type and its corresponding configurations into Deployment template
    # By default, will only inject an emptyDir
    #
    # Examples of hostPath and persistentVolumeClaim are:
    # volumes:
    #   - name: my-hostpath
    #     hostPath:
    #       path: /tmp/c3-tmp
    #       type: DirectoryOrCreate
    #   - name: my-pvc
    #     persistentVolumeClaim:
    #       claimName: ctptest
    #
    volumes:
      - name: environment-shared
        emptyDir: {}
    #
    # The below values are used to specify configs for persistent-volume-claim.yaml
    # Only applicable if a persistentVolumeClaim is specified in the volumes section above
    #
    # storageClassName: efs-sc
    # persistentVolumeName: infra-playground-dev-8
    #
    size: 1Gi
  # TODO PLAT-30261: documentation
  metadataStoreRoot: /metadata
  # TODO PLAT-31263
  jvmMaxMetaspaceSizeGb: 4
  #
  # The absolute path of the jemalloc library, relative to the container image filesystem. To prevent loading, set
  # ldPreload to false.
  #
  ldPreload: /libjemalloc.so.2
  ephemeralStorage:
    #
    # Volume name of emptyDir here
    #
    volumeName: ephemeral-storage-dir
    #
    # Vertx cacheDirBase
    #
    vertxCacheDir:
      #
      # mountPath for vertxCacheDir in empty dir
      #
      mountPath: /tmp/vertx-cache
      #
      # subPath for vertxCacheDir in empty dir
      #
      subPath: vertx-cache
    #
    # Vertx uploads Dir Base
    #
    vertxUploadsDir:
      #
      # mountPath for vertxUploadsDir in empty dir
      #
      mountPath: /tmp/vertx-file-uploads
      #
      # subPath for vertxUploadsDir in empty dir
      #
      subPath: vertx-file-uploads

  #
  # The pod filesystem mount path Downward API information will be exposed under.
  #
  downwardApi:
    mountPath: /etc/downward-api

c3-nginx-ingress:
  # disable creation of cluster role and cluster rolebinding to avoid conflict installing multiple c3 charts
  disableClusterRole: true
  controller:
    scope:
      enabled: true
