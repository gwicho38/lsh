# Default values for a C3 Cluster chart.
# This is a YAML-formatted file.

c3Cluster:
  #
  # The Kubernetes namespace to deploy the C3 cluster, including its dependencies, into. This value is declared here
  # for the sole reason of maintaining all C3 cluster configuration in a single place - the value.yaml file. Helm
  # does not look at it and does not use it any way. The C3 tooling layer reads it and drives Helm to install the
  # release into this namespace. If you need the namespace value in templates, use {{ .Release.Namespace }}
  #
  namespace: c3

  security:
    #
    # Roles and role bindings are deployed by default. However, there are situation when the RBAC configuration of the
    # namespace C3 cluster will operate under is externally set. In situations like these, deploying predefined roles
    # will only get in the way; it can be turned off by setting deployRoles to false.
    #
    deployRoles: true
    role: 'c3:role'
    # The Service Account name to use, if different from "default"
    #serviceAccountName: 'some-service-account'
    #
    # The directory where various PKI artifacts file will be exposed.
    #
    pkiDir: /usr/local/share/c3/server/pki
    #
    # Configure the pod security context
    #
    podSecurityContext:
      runAsUser: 4444 # If empty, a random UID will be generated
      runAsGroup: 4444 # If empty, a random GID will be generated
      fsGroup: 4444 # If empty, a random GID will be generated

      # An additional GID of 4444 is necessary for OpenShift, AKS etc. to grant write permissions
      # (e.g. write permissions to /usr/local/share/c3)
      supplementalGroups: [4444]
  #
  # env designates the C3 Environment ('dev', 'test', 'stage', 'prod' etc.) The name of the C3 Cluster is derived
  # from the C3 Environment name and the C3 Pod name, by separating these values by dash ('-') and concatenating
  # the strings.
  #
  env: 'dev'

  #
  # pod designates the C3 Pod - the set of C3 master and worker nodes working together and sharing resources
  # as part of the same C3 Cluster. The name of the C3 Cluster is derived from the C3 Environment name and the
  # C3 Pod name, by separating these values by dash ('-') and concatenating the strings.
  #
  pod: 'local'

  #
  # domain designates the domain to be used in the canonical url.
  #
  domain: 'test'

  #
  # The coordinates of the C3 server image used by the C3 node pods. Note that the "c3server" part of the
  # repository name is hardcoded into the Java code, so changing it here to something else will allow the
  # main node to start but not other nodes.
  #
  image:
    #registry: 'localhost:32000'
    registry: 'ci-artifacts.c3.ai'
    repository: 'c3server-k8s'
    tag: ''

    # Existing secret to pull the image from an authenticated repository.
    # Follow this naming convention outlined here, https://c3energy.atlassian.net/wiki/spaces/DATA/pages/1520274821/Configuring+C3.ai+Jupyter+Service+for+Azure+AKS+and+AWS+EKS#Update-existing-pull-secret
    # ("registry" + registryName).toLowerCase().replace( /[^a-z0-9]/g, '' );
    imagePullSecrets:
      name: ''

  #
  # Options to be sent to the c3-launch command via environment variables.
  #
  launchOptions:
    #
    # set to '1' to configure the C3 node JVM to start in debug mode. This configuration will apply to all nodes. The
    # JVM will listen by default on port 8787 and the port will be exposed by the "c3-debug" service.
    #
    debug: ''
    #
    # set to '1' to configure the C3 node JVM to suspend the boot process and wait for a debugger to attach. This
    # configuration will apply to all nodes. "debug" option (see above) must be set for this option to work.
    #
    suspend: ''
    debugMem: ''
    dryRun: ''
    gcLog: ''
    profile: ''
    verbose: ''

  # JVM configuration -- optional
  # set assumed container memory for JVM since there is no hard memory limit
  jvmMinMemFraction: ''
  jvmMaxMemFraction: ''
  # Set JVM min heap memory in terms of absolute value and override the derived value from jvmMinMemFraction when not ''.
  jvmMinHeapAbsolute: ''
  # Set JVM max heap memory in terms of absolute value and override the derived value from jvmMaxMemFraction when not ''.
  jvmMaxHeapAbsolute: ''
  jvmArgs: ''

  # AWS configuration -- optional
  aws:
    region: 'us-west-2'
    useMachineCreds: 'true'
    cloudSecurityId: 'AWS_CLOUD_SECURITY_ID_TO_BE_SET_DURING_INSTALLATION'
    artifactS3BucketRegion: 'us-west-2'
    artifactS3Bucket: 'c3--packagemanager'
    accessKey: 'AWS_ACCESS_KEY_TO_BE_SET_DURING_INSTALLATION'
    secretKey: 'AWS_SECRET_KEY_TO_BE_SET_DURING_INSTALLATION'
    testRegion: 'AWS_TEST_REGION_TO_BE_SET_DURING_INSTALLATION'
    testSecurityId: 'AWS_TEST_SECURITY_ID_TO_BE_SET_DURING_INSTALLATION'

  # HTTP(S) Proxy environment variables -- optional
  proxy:
    httpProxy: ''
    httpsProxy: ''
    noProxy: ''

  # TLS variables
  tls:
    internalCaInitContainerEnabled: false
    enabled: false
    trustStorePath: ''
    trustStorePassword: ''
    keyStorePath: ''
    keyStorePassword: ''
    certsDir: '/usr/local/certs'
    # The local filesystem directory to be mounted by the bootstrap node container and projected as the folder
    # where TLS truststore and keystore can be found
    # local:

  # Web Security variables
  websecurity:
    # Enable CORS response headers
    corsEnabled: false
    # Sets the value for the Vary Response Header
    varyHeader: ''
    # Enables the Cache-Control, Expires and Pragma Response headers to prevent caching of responses
    cacheControlHeaderEnabled: false
    # Enables the X-XSS-Protection Response header and sets the value to 1
    xssHeaderEnabled: false
    # Adds 'mode=block' to the X-XSS-Protection Response header value. Requires 'xssHeaderEnabled' to also be enabled.
    xssBlockModeEnabled: false
    # Sets the value for the X-Frame-Options Response Header, can be 'SAMEORIGIN' or 'DENY'
    xFrameOptionsHeader: ''
    # Sets the value for the Content-Security-Policy Response Header
    cspHeader: ''

  #
  # Server node resource
  # Bootstrap master node, memory requests, specified in Mi or Gi. Example '4096Mi'.
  # Bootstrap master node, CPU requests, specified in millicores. Example '1000m'.
  # Regular master nodes and worker nodes are sized based on instance type (e.g. 'C4_M8') or hardware profile,
  # if neither of these are provided this will be used as memory requests in regular master and worker nodes.
  # For more details see "Configure Compute Resource" section in the c3-cluster Helm Chart Configuration Guide.
  #
  resources:
    requests:
      cpu: '1100m'

  #
  # The C3_APPS_ROOT value.
  #
  c3AppsRoot: '/c3-shared-cluster/c3base'

  #
  # By default, start a master node, but allow the option of starting dual, tag, or bound instead
  #
  cloudFrameworkIdRole: 'm'

  #
  # We use busyboxy to run init containers that check on the state of dependencies and wait until the
  # dependency services are available. We found that some busybox releases are broken with respect to how
  # nslookup works. 1.28 works well.
  #
  busybox:
    registry: 'ci-artifacts.c3.ai'
    repository: 'ubi/ubi8-gov'
    tag: '8.4.211.1'

  # Enable local filesystem -- enable only for internal testing
  enableLocalFileSystem: 'false'

  #
  # This allows the use of non-secure cookies and server endpoints in k8s where we always run as a
  # real cluster (no local environment, etc). This setting should only be true in test environments.
  # The default value is 'false'
  #
  insecureAccess: true

  #
  # Istio configurations
  #
  istio:
    annotations:
      #  If set to true, the master and worker pods are deployed with the sidecar.istio.io/inject: "true" annotation.
      sidecarIstioIoInject: false
    enabled: false

  configFramework:
    config:
      # The container filesystem path where the config directory will be projected
      #
      mountPath: '/usr/local/share/c3/server/config'
      storage: 1Gi
      #
      # The local directory to be mounted by the bootstrap node container and projected as Config Framework
      # 'config' directory. This is a mandatory value, a C3 cluster will not properly operate without appropriate Config
      # Framework storage. Since it is unfeasible to come up with a reasonable default value, unless we're testing on a
      # single-nodes Kubernetes cluster, the value must be specified during installation, typically with
      # --config-url=<dir> command line option. For more details run 'c3-cluster install help'. The value will be
      # propagated to other C3 master and worker nodes created by the bootstrap node.
      #
      # local:
      #
      # The NFS share directory to be mounted by the bootstrap node container and projected as Config Framework 'config'
      # directory. This is a mandatory configuration element, a C3 cluster will not properly operate without appropriate
      # Config Framework storage. The configuration element must be specified during installation, typically with
      # --config-url=<NFS_share_URL> command line option, which will be translated to server and path. For more details
      # run 'c3-cluster install help'. These values will be propagated to other C3 master and worker nodes created by
      # the bootstrap node.
      #
      # nfs:
        #  server:
        #  path:
        #
        # Default value for CSI-provisioned NFS storage persistent volume claim. It is only relevant if CSI-provisioned
        # NFS storage is used. Can be overridden by the installation configuration, but this should not be normally
        # necessary. If the same PV is used to store both config and vault information, then the
        # persistentVolumeClaimName value should be the same as here in the 'vault' section below.
        #
        # persistentVolumeClaimName: 'c3-cf-pvc'

        #
        # Normally a persistent volume name needs not be specified, storage class should be sufficient. There are cases
        # though when the persistent volume name must be specified, and the following configuration element can be used
        # in those cases.
        #
        # persistentVolumeName: 'c3-cf-pv'
        #
        # A directory inside the CSI-exposed NFS file system root where the Config Framework config state will be
        # stored. It is only relevant if CSI-provisioned NFS storage is used. Can be overridden by the installation
        # configuration, but this should not be normally necessary, unless multiple C3 clusters share the same
        # persistent volume.
        #
        # subPath: 'c3-cf-config'
        #
        # A Kubernetes quantity specifying the minimum amount of storage required for configuration. Projects as
        # Persistent Volume Claim spec.resources.requests.storage.
        #
    vault:
      # The container filesystem path where the vault directory will be projected
      #
      mountPath: '/usr/local/share/c3/server/vault'
      #
      # A Kubernetes quantity specifying the minimum amount of storage required for vault. Projects as
      # Persistent Volume Claim spec.resources.requests.storage.
      #
      storage: 1Gi
      #
      # The local filesystem directory to be mounted by the bootstrap node container and projected as Config Framework
      # 'vault' directory. This is a mandatory value, a C3 cluster will not properly operate without appropriate Config
      # Framework storage. Since it is unfeasible to come up with a reasonable default value, unless we're testing on a
      # single-nodes Kubernetes cluster, the value must be specified during installation, typically with
      # --vault-url=<dir> command line option. For more details run 'c3-cluster install help'. The value will be
      # propagated to other C3 master and worker nodes created by the bootstrap node.
      #
      # local:
      #
      # The NFS share directory to be mounted by the bootstrap node container and projected as Config Framework 'vault'
      # directory. This is a mandatory configuration element, a C3 cluster will not properly operate without appropriate
      # Config Framework storage. The configuration element must be specified during installation, typically with
      # --vault-url=<NFS_share_URL> command line option, which will be translated to server and path. For more details
      # run 'c3-cluster install help'. These values will be propagated to other C3 master and worker nodes created by
      # the bootstrap node.
      #
      # nfs:
        #  server:
        #  path:
        #
        # Default value for CSI-provisioned NFS storage persistent volume claim. It is only relevant if CSI-provisioned
        # NFS storage is used. Can be overridden by the installation configuration, but this should not be normally
        # necessary. If the same PV is used to store both config and vault information, then the
        # persistentVolumeClaimName value should be the same here as in the 'config' section above.
        #
        # persistentVolumeClaimName: 'c3-cf-pvc'
        #
        # Normally a persistent volume name needs not be specified, storage class should be sufficient. There are cases
        # though when the persistent volume name must be specified, and the following configuration element can be used
        # in those cases.
        #
        # persistentVolumeName: 'c3-cf-pv'
        #
        # A directory inside the CSI-exposed NFS file system root where the Config Framework vault state will be
        # stored. It is only relevant if CSI-provisioned NFS storage is used. Can be overridden by the installation
        # configuration, but this should not be normally necessary, unless multiple C3 clusters share the same
        # persistent volume.
        #
        # subPath: 'c3-cf-vault'
    #
    # The container filesystem path directory that will contain the transient configuration during the installation
    # boot. The transient configuration is the file projection of the transient configuration secret. The full path
    # of the projection file is ${installationProjectionMountPath}/${installationProjectionFileName} (default value
    # "/usr/local/share/c3/server/installation/.transient-boot-configuration").
    #
    installationProjectionMountPath: '/usr/local/share/c3/server/installation'
    #
    # The name of the container filesystem file that will contain the projection of the transient configuration secret.
    # The full path of the projection file is ${installationProjectionMountPath}/${installationProjectionFileName}
    # (default value "/usr/local/share/c3/server/installation/.transient-boot-configuration").
    #
    installationProjectionFileName: '.cf-root'
    #
    # Dynamically generated by the installer at installation time, based on the content of the "root" element.
    # If no installation configuration is provided by the installer, the installation secret will be projected as
    # an empty file, which will be ignored by the boot manager.
    #
    #
    installationConfiguration: ''
    #
    # The root of Config Framework hierarchy. The element contains the canonical mapping of the Config Framework
    # hierarchical configuration (security sensitive and non-security-sensitive). This configuration will be projected
    # onto Config Framework types during the installation boot.
    #
    root:
      # TODO Remove after resolution of OP-19377
      KvStoreConfig:
        cassandra:
          username: cassandra
          password: c3cassandra
      db:
        JdbcStoreConfig:
          c3:
            credentials:
              database: c3
              datastore: postgres
              username: c3
              password: c3postgres
              port: 5432
              # serverEndpoint: <calculated dynamically>
              adminUsername: pgadminusername
              adminPassword: pgadminpassword
          secondaryPostgres:
            credentials:
              database: c3
              datastore: postgres
              username: c3
              password: c3postgres
              port: 5432
              # serverEndpoint: <calculated dynamically>
              adminUsername: pgadminusername
              adminPassword: pgadminpassword
          sql:
            credentials:
              database: c32
              datastore: postgres
              username: c32
              password: c3postgres
              port: 5432
              # serverEndpoint: <calculated dynamically>
              adminUsername: pgadminusername
              adminPassword: pgadminpassword

  #
  # Configuration of a local directory (for single-node Kubernetes clusters) or an NFS share (for multi-node Kubernetes
  # clusters) that will be projected into all C3 node (masters and workers) pods in the cluster. For semantics, see
  # below:
  #
  # TODO PLAT-24900
  #
  sharedFileSystem:
    # The name of the C3 node pod mount path: the local filesystem directory specified as "local" or the NFS share will
    # be projected in the C3 node pod as "mountPath"
    #
    mountPath: '/c3-cluster-shared'

    # A Kubernetes quantity specifying the minimum amount of storage required for the shared file system. Projects as
    # Persistent Volume Claim spec.resources.requests.storage.
    #
    storage: 1Gi

    #
    # The name of the local directory that will be mounted by the bootstrap master container, and then
    # subsequently by ordinary masters and workers and projected as shared directory.
    # Warning: using a local file system directory to provide shared file system space across cluster will only work
    # for single-node Kubernetes clusters (minikube). For multi-node clusters, use an NFS-based solution.
    #
    # local: '/tmp/c3-cluster-shared'

    # nfs:
      #
      # Default value for CSI-provisioned c3 shared filesystem persistent volume claim. It is only relevant if CSI-provisioned
      # NFS storage is used. Can be overridden by the installation configuration, but this should not be normally
      # necessary. If the same PV is used to store both config framework and shared filesystem information, then the
      # persistentVolumeClaimName value should be the same here as in the 'config' and 'vault' sections above.
      #
      # persistentVolumeClaimName: 'c3-cf-pvc'

      #
      # Normally a persistent volume name needs not be specified, storage class should be sufficient. There are cases
      # though when the persistent volume name must be specified, and the following configuration element can be used
      # in those cases.
      #
      # persistentVolumeName: 'c3-cf-pv'

      #
      # A directory inside the CSI-exposed NFS file system root where the c3 Shared Filesystem state will be
      # stored. It is only relevant if CSI-provisioned NFS storage is used. Can be overridden by the installation
      # configuration, but this should not be normally necessary.
      #
      # subPath: 'c3-cluster-shared'
      #

  #
  # Configuration for persistent storage utilizing persistentVolumeClaim.
  # If ephemeralStorage is enabled, files will be written to emptyDirs instead.
  #
  persistentStorage:
    persistentVolumeClaimName: 'c3-fs-pvc'
    #persistentVolumeName: 'c3-fs-pv'
    #storageClass: azurefiles
    storage: 1Gi

  #
  # Configuration for ephemeral storage utilizing emptyDir.
  # If ephemeralStorage is disabled, files will be written to persistentVolumeClaims instead.
  #
  ephemeralStorage:
    #
    # volume name of empty dir
    #
    volumeName: 'ephemeral-empty-dir'

    #
    #  If enabled use emptyDir, else use persistentVolumeClaim
    #
    enabled: true

    tmp:
      #
      # mountPath for tmp files in empty dir
      #
      mountPath: '/tmp'
      #
      # subPath for tmp files in empty dir
      #
      subPath: 'tmp'

    zookeeper:
      #
      # mountPath for zookeeper data files in empty dir
      #
      mountPath: '/usr/local/share/c3/zk/data'
      #
      # subPath for zookeeper data files in empty dir
      #
      subPath: 'zk/data'

    cache:
      #
      # mountPath for cache in empty dir
      #
      mountPath: '/usr/local/share/c3/cache'
      #
      # subPath for cache in empty dir
      #
      subPath: 'cache'

    c3Log:
      #
      # mountPath for c3server.log file in empty dir
      #
      mountPath: '/home/c3/c3log'
      #
      # subPath for c3server.log file in empty dir
      #
      subPath: 'c3log'

    serverConf:
      #
      # mountPath for server conf files in empty dir
      #
      mountPath: '/usr/local/share/c3/server/conf'
      #
      # subPath for server-conf files in empty dir
      #
      subPath: 'c3-cf-conf'

    npm:
      #
      # mountPath for npm modules in empty dir
      #
      mountPath: '/home/c3/.npm'
      #
      # subPath for npm modules in empty dir
      #
      subPath: '.npm'

    c3Tmp:
      #
      # mountPath for c3 tmp in empty dir
      #
      mountPath: '/usr/local/share/c3/tmp'
        #
      # subPath for c3 tmp in empty dir
      #
      subPath: 'tmp'

  #
  # Description for the K8s Services (web, debug), e.g., name, type.
  #
  service:
    web:
      #
      # The main (web) c3 cluster service name.
      #
      name: c3
      #
      # The c3 cluster web service type: NodePort, ClusterIP, LoadBalancer, etc.
      #
      type: ClusterIP
      #
      # The c3 cluster web service port definitions.
      #
      ports:
        - name: 'web'
          protocol: 'TCP'
          port: 8080
          targetPort: 8080
      #
      # This element can be used to inject arbitrary annotations into the metadata.annotations element of the web
      # Service manifest. Cloud provider-specific annotations, such as those specified as an example below, control
      # the type and the behavior of the cloud provider ingress load balancer. The common usage pattern is to provide
      # these annotations in the installation configuration file.
      #
      #annotations:
      # service.beta.kubernetes.io/aws-load-balancer-type: 'nlb'
      # service.beta.kubernetes.io/aws-load-balancer-security-groups: 'sg-00000000000000000'
    debug:
      #
      # The c3 bootstrap node debug service name. It was separated from the main (web) service because we need this
      # service to route to endpoint immediately, without waiting for a readiness probe to pass. This allows debugging
      # the boot process.
      #
      name: c3-debug
      #
      # The c3 cluster debug service type: NodePort, ClusterIP, LoadBalancer, etc.
      #
      type: ClusterIP
      #
      # The c3 cluster debug service port definitions.
      #
      ports:
        - name: 'debug'
          protocol: 'TCP'
          port: 8787
          targetPort: 8787
  #
  # Zookeeper Client Configuration - this propagates to the ZooKeeper client embedded within the C3 server.
  # Also see zookeeper.env.ZK_MAX_SESSION_TIMEOUT below.
  #
  zookeeperClient:

    # Configure when the Zookeeper cluster not deployed as part of the c3-cluster Helm chart, but managed independently
    # hosts: 'zookeeper'

    maxSessionTimeout: '18000000'
    username: ''
    password: ''

  # Port used for canonical URL (used with nginx ingress controller locally)
  # canonicalUrlPortOverride: 80

  #
  # The ingress configuration.
  #
  ingress:
    #
    # Enables the ingress on c3 cluster.
    #
    enabled: true

    #
    # Class of ingress controller to use in k8s Ingress, e.g.: nginx, alb
    #
    class: nginx

    #
    # Install helm chart that will bring nginx-controller pods
    #
    install-nginx-controller: true

    #
    # Install helm chart that will bring aws-alb-ingress-controller pods
    #
    install-aws-alb-ingress-controller: false

    annotations:
      nginx.ingress.kubernetes.io/rewrite-target: /$1
      nginx.ingress.kubernetes.io/proxy-connect-timeout: "60"
      nginx.ingress.kubernetes.io/proxy-send-timeout: "3600"
      nginx.ingress.kubernetes.io/proxy-read-timeout: "3600"
      nginx.ingress.kubernetes.io/proxy-body-size: "250m" # maximum payload size, e.g. zipped packages for a tag, a client can send

      #
      # provide with overlay for AWS ALB
      #
      # alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:us-west-2:999999999999:certificate/99999999-9999-9999-9999-999999999999,arn:aws:acm:us-west-2:999999999999:certificate/88888888-8888-8888-8888-888888888888
      # alb.ingress.kubernetes.io/scheme: internal
      # alb.ingress.kubernetes.io/inbound-cidrs: 10.0.0.0/8
      # alb.ingress.kubernetes.io/security-group-inbound-cidrs: 10.0.0.0/8
      # alb.ingress.kubernetes.io/target-type: instance
      # alb.ingress.kubernetes.io/listen-ports: '[{"HTTP": 80}, {"HTTPS":443}]'
      # alb.ingress.kubernetes.io/actions.ssl-redirect: '{"Type": "redirect", "RedirectConfig": { "Protocol": "HTTPS", "Port": "443", "StatusCode": "HTTP_301"}}'

    serviceName: c3
    servicePort: 8080

    # rules:
      #
      # The hosts definitions of ingress rules.
      #
      # - host:

        #
        # The backend definitions of ingress rules.
        #
        # backend:
          #
          # Service behind Ingress. Note this needs to match c3Cluster.service.web.name.
          #
          # serviceName: c3
          #
          # Service port. Needs to be defined by service. Refer to c3Cluster.service.web.ports.
          #
          # servicePort: 8080

    tls:
      #
      # If enabled, supply the cert & key below and create the secret to store tls.
      #
      enabled: false

      #
      # The secret that includes certificate and key.
      #
      secret:
        #
        # The secret name that needs to be created with `kubectl create secret tls..`
        #
        name: c3-tls-secret

        #
        # The tls cert in base64 encoded, supply value here if ingress enabled.
        #
        cert: ''

        #
        # The tls key in base64 encoded, supply value here if ingress enabled.
        #
        key: ''

        #
        # The hosts that secret applies to.
        #
        hosts: ''

  cassandra:
    #knownNode: 'cassandra'
    username: ''
    password: ''

  #
  # The pod filesystem mount path Downward API information will be exposed under.
  #
  downwardApi:
    mountPath: /etc/podinfo

  # Configuration for External Vault.
  #
  externalVault:
    #
    # External Vault providers, e.g., HashiCorp, Blob Store Storage
    #
    kind: ''
    #
    # The Vault url to connect.
    #
    url: ''
    #
    # The auth mechanism for External Vault. If configured kubernetes, a few resources will be created for vault auth.
    #
    auth: ''
    #
    # The role binds to the policy in the Vault. Supplied by Vault administrator.
    #
    role: ''
    #
    # Config root path in Config Framework.
    #
    configRoot: ''
    #
    # Owner of External Vault where config will be stored, by default this value is "c3dev"
    #
    configOwner:
    #
    # Cloud region of External Vault where config will be stored, by default this value is "us-west-2".
    #
    configRegion:
    #
    # Vault root path in Config Framework.
    #
    vaultRoot: ''
    #
    # Cloud region of External Vault where vault will be stored, by default this value is "us-west-2".
    #
    vaultRegion:
    #
    # If enabled, Vault is used as CA for the master and worker nodes.
    #
    isClusterCA: false
    #
    # If isClusterCA flag is enabled, the Kubernetes secret whose key is 'vault' will be persisted to the specified directory.
    #
    vaultTokenDir: /usr/local/vault_token

  #
  # Conda configuration for master and worker nodes
  #
  conda:
    #
    # Configures conda with the specified custom channels. If no value is not set, conda configuration will not be attempted.
    #
    #channelUrls:
    #  - https://example1.com/conda
    #  - https://example2.com/conda
    #
    # Conda configuration files
    #
    configFiles:
      - /root/.condarc
      - /home/c3/.condarc
      - /opt/conda/.condarc
  #
  # pip configuration for master and worker nodes
  #
  pip:
    #
    # The pip index URL. If this value is not set, pip configuration will not be attempted.
    #
    #indexUrl: https://example.com/pip
    configFiles:
      - /home/c3/.pip/pip.conf
  #
  # npm configuration for master and worker nodes
  #
  npm:
    #
    # The npm registry URL. If this value is not set, npm configuration will not be attempted.
    #
    #registryUrl: https://example.com/npm
    configFiles:
      - /home/c3/.npmrc

  kafka:
    #
    # The name of the truststore file.
    #
    truststoreFileName: kafka-truststore.jks
    #
    # base64-encoded content of the truststore to be provided in the installation overlay. Must be enclosed in
    # single-quotes as shown below. If this value is missing in the overlay, or it is set to false or an empty value,
    # the truststore will not be projected in containers.
    #
    # truststoreContent: '/u3+7QAAAAIAAAAB....DM0/JqL'

zookeeper:
  #
  # set this to false if the Zookeeper cluster not deployed as part of the c3-cluster Helm chart, but managed
  # independently
  #
  enabled: true
  #
  # Zookeeper version notes. The zookeeper chart 1.3.3 deploys a "gcr.io/google_samples/k8szk:v3" Zookeper image, which
  # contains Zookeeper 3.4.10, and that is too old. That's the highest we get from gcr.io/google_samples/k8sz. The next
  # Zookeeper chart version is 2.0.0, and that comes by default with Zookeeper 3.5.5. An initial attempt to make it work
  # with Zookeeper 3.4.14 was unsuccessful.
  #
  image:
    # O/S chart configuration
    registry: ci-artifacts.c3.ai
    repository: bitnami/c3-zookeeper
    tag: "3.5.6"
    # OPS chart configuration
#    registry: registry.c3.ai
#    repository: zookeeper
#    tag: "3.5.8.6"
#    pullSecrets: registryc3ai

  #
  # The number of Zookeeper nodes in a quorum.
  #
  replicaCount: 3

  #
  # Allow Zookeeper to listen for connections from its peers on all available IP addresses.
  #
  #
  listenOnAllIPs: false

  #
  # Configure ZooKeeper with a custom zoo.conf file
  #
  #config:
    #
    # See c3Cluster.zookeeperClient.maxSessionTimeout
    #
    #maxSessionTimeout: '18000000'

  #
  # OpenShift configuration
  #
#  securityContext:
#    runAsUser: 1000600001
#    runAsGroup: 1000600001
#  containerSecurityContext:
#    readOnlyRootFilesystem: true
#    allowPrivilegeEscalation: false
#    runAsUser: 1000600001

#
# Note: configuration values for persistence components are declared in the values.yaml file that is packages with
# the c3-persistence chart. They can be overridden here but that is normally not necessary.
#
c3-persistence:
  vault:
    enabled: false
  cassandra:
    enabled: true
  postgresql:
    enabled: true
aws-alb-ingress-controller:
  #
  # Set cluster name if you want to do deploy using ALB
  #
  clusterName: ''
  autoDiscoverAwsRegion: true
  autoDiscoverAwsVpcID: true
